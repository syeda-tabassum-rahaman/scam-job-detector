### Performance metrics ###

*** Note: Accuracy, Precision, Recall, F1 → measured at one threshold (often 0.5).

AP → measures performance across all thresholds.***

1. Average Precision (AP)

Average Precision summarizes a Precision–Recall (PR) curve as the area under the curve,
 using a step-wise (piecewise constant) interpolation.

AP = Sum over i ( (R_i - R_{i-1}) * P_i )

Where:

P_i = precision at threshold i
R_i = recall at threshold i
(R_i - R_{i-1}) = change in recall when moving to threshold i
The summation runs over all unique prediction score thresholds (sorted in descending order)

2. Recall (Sensitivity, True Positive Rate)

Measures how well the model identifies actual positives.
Recall = TP / (TP + FN)

3. Precision (Positive Predictive Value)

Measures how reliable the model's positive predictions are.
Precision = TP / (TP + FP)

4. Balanced Accuracy (binary case)

Average recall of positive and negative classes.
Balanced Accuracy = 0.5 * ( TP / (TP + FN) + TN / (TN + FP) )

5. F1 Score

Harmonic mean of precision and recall.
F1 = 2 * ( Precision * Recall ) / ( Precision + Recall )
