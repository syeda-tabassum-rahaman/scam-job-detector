# ğŸ•µï¸â€â™€ï¸ Fake Job Scam Detector
Identify fraudulent job postings using ML, NLP and deep learning.

---

# ğŸ“Œ Description
This project aims to detect fraudulent job postings using:

- Logistic Regression + TF-IDF
- BERT embeddings (optional)
- XGBoost (optional)
- A full preprocessing pipeline (One-Hot + Ordinal + TF-IDF)
- FastAPI deployment
- Streamlit UI

The goal is to assist job seekers by flagging potentially fraudulent jobs before they waste time or expose sensitive data.

---

# ğŸ“‚ Repository Structure
```bash
scam-job-detector/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ fast.py                 # FastAPI application
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ build/                      # Auto-generated during packaging
â”‚   â””â”€â”€ lib/
â”‚       â””â”€â”€ scam_job_detector/
â”‚           â”œâ”€â”€ ML_logic/
â”‚           â”œâ”€â”€ params.py
â”‚           â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.dill              # Main trained model (LogReg)
â”‚   â”œâ”€â”€ model_logreg.dill       # Optional alternative model
â”‚   â”œâ”€â”€ model_xgb.dill          # Optional XGBoost model
â”‚   â””â”€â”€ preprocessor.dill       # Saved preprocessing pipeline
â”‚
â”œâ”€â”€ notebooks/                  # All exploratory Jupyter notebooks
â”‚   â”œâ”€â”€ data_inspection_syeda.ipynb
â”‚   â”œâ”€â”€ FirstInspection_Lars.ipynb
â”‚   â””â”€â”€ gilles_eda.ipynb
â”‚
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ fake_job_postings.csv   # Original dataset
â”‚   â””â”€â”€ data_cleaned.csv        # Pre-cleaned dataset (optional)
â”‚
â”œâ”€â”€ scam_job_detector/
â”‚   â”œâ”€â”€ ML_logic/
â”‚   â”‚   â”œâ”€â”€ data.py             # Text cleaning + feature engineering
â”‚   â”‚   â”œâ”€â”€ model.py            # GridSearch + training + saving
â”‚   â”‚   â”œâ”€â”€ preprocessor.py     # ColumnTransformer (OHE, Ordinal, TF-IDF)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ params.py               # Global parameters (if used)
â”‚   â”œâ”€â”€ utils.py                # Utility functions
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ requirements.txt            # Package dependencies
â”œâ”€â”€ requirements_dev.txt        # Dev dependencies (linting, formatting)
â”œâ”€â”€ Dockerfile                  # Docker runtime definition
â”œâ”€â”€ Makefile                    # CLI shortcuts for training, API, etc.
â”œâ”€â”€ setup.py                    # Packaging config
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ tests/                      # Unit tests
```

---

# ğŸ§¹ Data Cleaning

Key preprocessing steps applied:

- Lowercasing all text
- Removing punctuation and numbers
- Removing English stopwords
- Lemmatization
- Filling missing text with `"missing value"`
- Creating binary indicators for missing important columns
- Extracting `country` from the `location` string
- Dropping irrelevant columns (`job_id`, `department`, `salary_range`, `location`)

The heavy text cleaning (tokenization, lemmatization, stopwords) is performed once in `data.py`.

---

# ğŸ”§ Preprocessing Pipeline (sklearn)

Using a `ColumnTransformer` combining:

- **OneHotEncoder** â†’ categorical columns
- **OrdinalEncoder** â†’ ordered columns like experience & education
- **FunctionTransformer** â†’ combines 5 text columns into one
- **TfidfVectorizer** â†’ numeric vectors from text
- **SimpleImputer** â†’ handles missing values safely

All preprocessing is fitted only on the **training split** to avoid data leakage.

---

# ğŸ¤– Model Training

The baseline model uses **Logistic Regression** with:

- `solver="liblinear"`
- class imbalance handling (`class_weight="balanced"`)
- GridSearchCV with 5-fold stratification
- `average_precision` as the scoring metric

Metrics evaluated:

- Precision
- Recall
- F1 score
- Balanced accuracy

Final model is saved as:

- `models/model.dill` (trained classifier)
- `models/preprocessor.dill` (feature engineering pipeline)

The FastAPI service loads both artifacts during startup, applies the
preprocessor to incoming requests, and returns predictions from the model.

---

# ğŸ”§ Setup Instructions

### Install Python environment
```bash
pyenv install 3.10.6
pyenv virtualenv 3.10.6 scam_job_detector
pyenv local scam_job_detector
pip install --upgrade pip
pip install -r requirements.txt

# Place dataset at:
# raw_data/fake_job_postings.csv

# Clean the dataset and generate data_cleaned.csv
python -m scam_job_detector.ML_logic.data

python -m scam_job_detector.ML_logic.model
# Outputs:
# models/model.dill
# models/preprocessor.dill

uvicorn api.fast:app --reload
# Visit:
# http://127.0.0.1:8000
# http://127.0.0.1:8000/docs

streamlit run streamlit/app.py

---

# ğŸ§ª Usage

### Load trained model
```python
from scam_job_detector.ML_logic.model import load_model
model = load_model()
