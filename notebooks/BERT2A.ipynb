{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbb9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f9206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "805/805 [==============================] - 105s 126ms/step - loss: 0.2023 - accuracy: 0.9527 - val_loss: 0.1273 - val_accuracy: 0.9644\n",
      "Epoch 2/10\n",
      "805/805 [==============================] - 95s 117ms/step - loss: 0.0953 - accuracy: 0.9706 - val_loss: 0.0627 - val_accuracy: 0.9818\n",
      "Epoch 3/10\n",
      "805/805 [==============================] - 94s 117ms/step - loss: 0.0523 - accuracy: 0.9840 - val_loss: 0.0584 - val_accuracy: 0.9769\n",
      "Epoch 4/10\n",
      "805/805 [==============================] - 93s 115ms/step - loss: 0.0361 - accuracy: 0.9897 - val_loss: 0.0613 - val_accuracy: 0.9797\n",
      "Epoch 5/10\n",
      "805/805 [==============================] - 94s 116ms/step - loss: 0.0299 - accuracy: 0.9911 - val_loss: 0.0578 - val_accuracy: 0.9811\n",
      "Epoch 6/10\n",
      "805/805 [==============================] - 94s 117ms/step - loss: 0.0203 - accuracy: 0.9944 - val_loss: 0.0642 - val_accuracy: 0.9832\n",
      "Epoch 7/10\n",
      "805/805 [==============================] - 94s 116ms/step - loss: 0.0154 - accuracy: 0.9959 - val_loss: 0.0584 - val_accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "805/805 [==============================] - 96s 119ms/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.0648 - val_accuracy: 0.9846\n",
      "Epoch 9/10\n",
      "805/805 [==============================] - 98s 122ms/step - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.0685 - val_accuracy: 0.9818\n",
      "Epoch 10/10\n",
      "805/805 [==============================] - 98s 122ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.0688 - val_accuracy: 0.9846\n",
      "112/112 [==============================] - 8s 72ms/step - loss: 0.0516 - accuracy: 0.9871\n",
      "Test Accuracy: 0.9871364831924438\n",
      "112/112 [==============================] - 9s 74ms/step\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Data Targets\n",
    "# -----------------------------\n",
    "# read file\n",
    "data_path = '/Users/jonathankipping/code/syeda-tabassum-rahaman/scam-job-detector/raw_data/data_cleaned.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "X = df.drop(columns=[\"fraudulent\"])\n",
    "y = df[\"fraudulent\"]\n",
    "# -----------------------------\n",
    "# Combine text columns\n",
    "# -----------------------------\n",
    "text_columns = [\n",
    "    \"title\",\n",
    "    \"company_profile\",\n",
    "    \"description\",\n",
    "    \"requirements\",\n",
    "    \"benefits\"\n",
    "]\n",
    "combined_text = X[text_columns].fillna(\"\").agg(\" \".join, axis=1).to_list()\n",
    "# -----------------------------\n",
    "# Train-test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_text, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "# -----------------------------\n",
    "# Tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "train_tokens = tokenizer(\n",
    "    X_train,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "test_tokens = tokenizer(\n",
    "    X_test,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "# -----------------------------\n",
    "# Load BERT Tiny model\n",
    "# -----------------------------\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels=2,\n",
    "    from_pt=True\n",
    ")\n",
    "# -----------------------------\n",
    "# Compile Model\n",
    "# -----------------------------\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# -----------------------------\n",
    "# Fit Model\n",
    "# -----------------------------\n",
    "history = model.fit(\n",
    "    {\n",
    "        \"input_ids\": train_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": train_tokens[\"attention_mask\"],\n",
    "    },\n",
    "    y_train.values,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=16\n",
    ")\n",
    "# -----------------------------\n",
    "# Evaluate\n",
    "# -----------------------------\n",
    "test_loss, test_acc = model.evaluate(\n",
    "    {\n",
    "        \"input_ids\": test_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": test_tokens[\"attention_mask\"],\n",
    "    },\n",
    "    y_test.values\n",
    ")\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Predict on test set\n",
    "pred_logits = model.predict(\n",
    "    {\n",
    "        \"input_ids\": test_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": test_tokens[\"attention_mask\"],\n",
    "    }\n",
    ").logits\n",
    "\n",
    "# Convert logits â†’ class labels\n",
    "y_prob = pred_logits[:,1]\n",
    "\n",
    "y_prob\n",
    "pd.DataFrame(y_prob).to_csv('../raw_data/bert_probas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f137e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_prob).to_csv('../raw_data/bert_probas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98ffb05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 9s 77ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[ 3.6307786, -3.7005801],\n",
       "       [ 3.925928 , -3.991476 ],\n",
       "       [ 3.881991 , -3.9434168],\n",
       "       ...,\n",
       "       [ 3.920732 , -3.9818761],\n",
       "       [ 3.913963 , -3.9754841],\n",
       "       [ 3.9302251, -3.9888659]], shape=(3576, 2), dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test set\n",
    "pred = model.predict(\n",
    "    {\n",
    "        \"input_ids\": test_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": test_tokens[\"attention_mask\"],\n",
    "    }\n",
    ")\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d8677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scam_job_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
