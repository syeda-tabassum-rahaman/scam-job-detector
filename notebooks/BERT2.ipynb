{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbb9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f9206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "805/805 [==============================] - 95s 116ms/step - loss: 0.2110 - accuracy: 0.9508 - val_loss: 0.1240 - val_accuracy: 0.9665\n",
      "Epoch 2/50\n",
      "805/805 [==============================] - 88s 109ms/step - loss: 0.0971 - accuracy: 0.9689 - val_loss: 0.0785 - val_accuracy: 0.9776\n",
      "Epoch 3/50\n",
      "805/805 [==============================] - 87s 108ms/step - loss: 0.0533 - accuracy: 0.9830 - val_loss: 0.0604 - val_accuracy: 0.9797\n",
      "Epoch 4/50\n",
      "805/805 [==============================] - 89s 110ms/step - loss: 0.0351 - accuracy: 0.9897 - val_loss: 0.0572 - val_accuracy: 0.9825\n",
      "Epoch 5/50\n",
      "805/805 [==============================] - 89s 110ms/step - loss: 0.0260 - accuracy: 0.9933 - val_loss: 0.0506 - val_accuracy: 0.9832\n",
      "Epoch 6/50\n",
      "805/805 [==============================] - 89s 111ms/step - loss: 0.0189 - accuracy: 0.9953 - val_loss: 0.0618 - val_accuracy: 0.9832\n",
      "Epoch 7/50\n",
      "805/805 [==============================] - 88s 109ms/step - loss: 0.0154 - accuracy: 0.9964 - val_loss: 0.0541 - val_accuracy: 0.9874\n",
      "Epoch 8/50\n",
      "805/805 [==============================] - 88s 109ms/step - loss: 0.0114 - accuracy: 0.9970 - val_loss: 0.0629 - val_accuracy: 0.9839\n",
      "Epoch 9/50\n",
      "805/805 [==============================] - 88s 109ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.0658 - val_accuracy: 0.9860\n",
      "Epoch 10/50\n",
      "805/805 [==============================] - 88s 110ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0486 - val_accuracy: 0.9867\n",
      "112/112 [==============================] - 7s 66ms/step - loss: 0.0596 - accuracy: 0.9883\n",
      "Test Accuracy: 0.9882550239562988\n",
      "112/112 [==============================] - 7s 64ms/step\n",
      "\n",
      "Model Performance\n",
      "Recall:             0.7977\n",
      "Precision:          0.9517\n",
      "Balanced Accuracy:  0.8978\n",
      "F1 Score:           0.8679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Data Targets\n",
    "# -----------------------------\n",
    "# read file\n",
    "data_path = '/Users/jonathankipping/code/syeda-tabassum-rahaman/scam-job-detector/raw_data/data_cleaned.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "X = df.drop(columns=[\"fraudulent\"])\n",
    "y = df[\"fraudulent\"]\n",
    "# -----------------------------\n",
    "# Combine text columns\n",
    "# -----------------------------\n",
    "text_columns = [\n",
    "    \"title\",\n",
    "    \"company_profile\",\n",
    "    \"description\",\n",
    "    \"requirements\",\n",
    "    \"benefits\"\n",
    "]\n",
    "combined_text = X[text_columns].fillna(\"\").agg(\" \".join, axis=1).to_list()\n",
    "# -----------------------------\n",
    "# Train-test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_text, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "# -----------------------------\n",
    "# Tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "train_tokens = tokenizer(\n",
    "    X_train,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "test_tokens = tokenizer(\n",
    "    X_test,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "# -----------------------------\n",
    "# Load BERT Tiny model\n",
    "# -----------------------------\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\",\n",
    "    num_labels=2,\n",
    "    from_pt=True\n",
    ")\n",
    "# -----------------------------\n",
    "# Compile Model\n",
    "# -----------------------------\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Early Stopping\n",
    "# -----------------------------\n",
    "\n",
    "es =  tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# -----------------------------\n",
    "# Fit Model\n",
    "# -----------------------------\n",
    "history = model.fit(\n",
    "    {\n",
    "        \"input_ids\": train_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": train_tokens[\"attention_mask\"],\n",
    "    },\n",
    "    y_train.values,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=[es],\n",
    ")\n",
    "# -----------------------------\n",
    "# Evaluate\n",
    "# -----------------------------\n",
    "test_loss, test_acc = model.evaluate(\n",
    "    {\n",
    "        \"input_ids\": test_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": test_tokens[\"attention_mask\"],\n",
    "    },\n",
    "    y_test.values\n",
    ")\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Predict on test set\n",
    "pred_logits = model.predict(\n",
    "    {\n",
    "        \"input_ids\": test_tokens[\"input_ids\"],\n",
    "        \"attention_mask\": test_tokens[\"attention_mask\"],\n",
    "    }\n",
    ").logits\n",
    "\n",
    "# Convert logits â†’ class labels\n",
    "y_pred = np.argmax(pred_logits, axis=1)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"\"\"\n",
    "Model Performance\n",
    "Recall:             {recall_score(y_test, y_pred):.4f}\n",
    "Precision:          {precision_score(y_test, y_pred):.4f}\n",
    "Balanced Accuracy:  {balanced_accuracy_score(y_test, y_pred):.4f}\n",
    "F1 Score:           {f1_score(y_test, y_pred):.4f}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f137e69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance\n",
      "Recall:             0.7977\n",
      "Precision:          0.9517\n",
      "Balanced Accuracy:  0.8978\n",
      "F1 Score:           0.8679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Model Performance\n",
    "Recall:             {recall_score(y_test, y_pred):.4f}\n",
    "Precision:          {precision_score(y_test, y_pred):.4f}\n",
    "Balanced Accuracy:  {balanced_accuracy_score(y_test, y_pred):.4f}\n",
    "F1 Score:           {f1_score(y_test, y_pred):.4f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffb05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scam_job_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
