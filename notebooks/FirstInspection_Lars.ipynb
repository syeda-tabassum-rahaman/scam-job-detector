{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d84a504",
   "metadata": {},
   "source": [
    "## Fake Data Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa2fbd",
   "metadata": {},
   "source": [
    "### About The Data\n",
    "This dataset contains 18K job descriptions out of which about 800 are fake. The data consists of both textual information and meta-information about the jobs. The dataset can be used to create classification models which can learn the job descriptions which are fraudulent. A small proportion of these descriptions are fake or scam which can be identified by the column \"fraudulent\". \n",
    "\n",
    "The data is provide by the University of the Aegean | Laboratory of Information & Communication Systems Security\n",
    "\n",
    "http://emscad.samos.aegean.gr/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb063a9",
   "metadata": {},
   "source": [
    "## Dictonary:\n",
    "-  job_id: Unique ID (int64)\n",
    "-  title: Title of job description (str)\n",
    "-  location: Geographical location of the job ad (Example: US, NY, New York)\n",
    "-  department: Corporate department (e.g. Marketing, Success, Sales, ANDROIDPIT, ...)\n",
    "-  salary_range: Indicative salary range (e.g. 50,000-60,000 ($))\n",
    "-  company_profile: A brief company description.\n",
    "-  description: The details description of the job ad.\n",
    "-  requirements: Enlisted requirements for the job opening.\n",
    "-  benefits: Enlisted offered benefits by the employer.\n",
    "-  telecommuting: True for telecommuting positions. --> remote or not\n",
    "-  has_company_logo: True if company logo is present.\n",
    "-  has_questions: True if screening questions are present.\n",
    "-  employment_type: Type of emplyment (e.g. Full-type, Part-time, Contract, etc.)\n",
    "-  required_experience: Required Experience (e.g. Executive, Entry level, Intern, etc.)\n",
    "-  required_education: Required Education (e.g. Doctorate, Master’s Degree, Bachelor, etc)\n",
    "-  industry: Industry (e.g. Automotive, IT, Health care, Real estate, etc.)\n",
    "-  function: Position as function in the company (e.g. Consulting, Engineering, Research, Sales etc.)\n",
    "-  fraudulent: Classifcation target (0, 1)\n",
    "\n",
    "\n",
    "# Columns to do:\n",
    "## string manipulation\n",
    "- title\n",
    "- company_profile\n",
    "- description\n",
    "- requirements\n",
    "- benefits\n",
    "\n",
    "## one-hot encode\n",
    "- location (3.105) - cities and countries --> remove\n",
    "    - countries = 90 (346 is NA) --> keep\n",
    "- industry (groups = 131) --> boolean mask (group all with less than 30 into one group) --> create category with missings\n",
    "- function (groups = 37)  --> create category with missings\n",
    "\n",
    "- employment_type (groups = 5) \n",
    "- required_experience (groups = 7)\n",
    "- required_education (groups = 13)\n",
    "\n",
    "## binary (no mising)\n",
    "- telecommuting\n",
    "- has_company_logo\n",
    "- has_questions\n",
    "- salary_range --> turn into binary (has salary range or not)\n",
    "- department (groups = 1337) --> binary \n",
    "\n",
    "## target\n",
    "- fraudulent (binary)\n",
    "\n",
    "## dropping\n",
    "department (groups = 1337) --> boolean mask (group all with less than 30 into one group) --> drop for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277ea0f",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- How to impute data with more sophisticated methods?\n",
    "- How to examine whether values are true NAs or just the result of company size?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12b85c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies as needed:\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c665554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "data_path = '/home/lars/code/syeda-tabassum-rahaman/scam-job-detector/raw_data/fake_job_postings.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "# print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd14541",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "Shape: {df.shape}\n",
    "Size: {df.size}\n",
    "Unique Ids: {df.job_id.nunique()}\n",
    "Locations: {df.location.nunique()}\n",
    "Departments: {df.department.nunique()}; {df.department.unique()}\n",
    "Salary Range: {df.salary_range.describe()}\n",
    "Column names: {df.columns}\n",
    "\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f729ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean raw data by\n",
    "    - Creating new features for columns with missing values above >30% as binary features: missing = 0, not missing = 1\n",
    "    - Cleaning text data by removing stopwords, digits, lamatizing, etc.\n",
    "    - \n",
    "    \"\"\"\n",
    "    def preprocessing(sentence):\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # remove punctuation\n",
    "        for punctuation in string.punctuation:\n",
    "            sentence = sentence.replace(punctuation, '')\n",
    "\n",
    "        # set to lowercase\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # remove numbers\n",
    "        for char in string.digits:\n",
    "            sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "\n",
    "        # tokenize\n",
    "        tokens = word_tokenize(sentence)\n",
    "\n",
    "        # removing stop words\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # lemmatize\n",
    "        tokens = [WordNetLemmatizer().lemmatize(word, pos='v') for word in tokens]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # path = Path('.')\n",
    "    df = pd.read_csv('../raw_data/fake_job_postings.csv')\n",
    "    print('dataset loaded')\n",
    "\n",
    "    # Creating binary columns for missing values:\n",
    "    df['department_binary'] = df['department'].map(lambda x: 0 if pd.isna(x) else 1)\n",
    "    \n",
    "    df['salary_range_binary'] = df['salary_range'].map(lambda x: 0 if pd.isna(x) else 1)\n",
    "    \n",
    "    # Clean text data\n",
    "    cols = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df[col] = df[col].fillna('missing value')\n",
    "\n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(preprocessing)\n",
    "    \n",
    "    # extracting country ID\n",
    "    df['country'] = df['location'].astype(str).apply(lambda x: x.split(',')[0])\n",
    "\n",
    "    # dropping columns\n",
    "    df.drop(columns=['salary_range', 'department', 'location', 'job_id'], inplace=True)\n",
    "    \n",
    "\n",
    "    print(\"✅ data cleaned\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "743e51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "# catagorical columns for One-Hot Encoding\n",
    "categorical_columns = [\n",
    "    'country',\n",
    "    'industry',\n",
    "    'function',\n",
    "    'employment_type'\n",
    "]\n",
    "# ordinal columns for Ordinal Encoding\n",
    "ordinal_columns = [\n",
    "    'required_experience',\n",
    "    'required_education'\n",
    "]\n",
    "#binary columns for binary encoding\n",
    "binary_columns = ['has_company_logo', 'has_questions', 'department_binary', 'salary_range_binary']\n",
    "\n",
    "#text columns for TF-IDF Vectorizer\n",
    "text_columns = [\n",
    "        'title',\n",
    "        'company_profile',\n",
    "        'description',\n",
    "        'requirements',\n",
    "        'benefits'\n",
    "]\n",
    "\n",
    "#reference lists for ordinal encoding\n",
    "experience_order = [\n",
    "    \"Not Applicable\",\n",
    "    \"Unknown\",\n",
    "    \"Internship\",\n",
    "    \"Entry level\",\n",
    "    \"Associate\",\n",
    "    \"Mid-Senior level\",\n",
    "    \"Director\",\n",
    "    \"Executive\"\n",
    "]\n",
    "\n",
    "education_order = [\n",
    "    \"Unknown\",\n",
    "    \"High School or equivalent\",\n",
    "    \"Vocational\",\n",
    "    \"Certification\",\n",
    "    \"Some College Coursework Completed\",\n",
    "    \"Associate Degree\",\n",
    "    \"Bachelor's Degree\",\n",
    "    \"Professional\",\n",
    "    \"Master's Degree\"\n",
    "]\n",
    "\n",
    "\n",
    "# preprocessor pipeline\n",
    "def preprocessing_pipeline() -> ColumnTransformer:\n",
    "\n",
    "    cat_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "        OneHotEncoder(handle_unknown='ignore')\n",
    "    )\n",
    "    ordinal_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "        OrdinalEncoder(\n",
    "        categories=[experience_order, education_order],\n",
    "        handle_unknown=\"use_encoded_value\",\n",
    "        unknown_value=-1)\n",
    "    )\n",
    "    binary_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy='most_frequent', fill_value=0),\n",
    "        OneHotEncoder(handle_unknown='ignore')\n",
    "    )\n",
    "\n",
    "    def combine_text(X):\n",
    "        return X[text_columns].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    \n",
    "    text_transformer = make_pipeline(\n",
    "        FunctionTransformer(combine_text, validate=False),\n",
    "        TfidfVectorizer(max_features=5000)\n",
    "    )\n",
    "\n",
    "    \n",
    "    preprocessor = make_column_transformer(\n",
    "        (cat_transformer, categorical_columns),\n",
    "        (ordinal_transformer, ordinal_columns),\n",
    "        (binary_transformer, binary_columns),\n",
    "        (text_transformer, text_columns)\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "# train preprocessor pipeline\n",
    "def train_preprocessor(X_train: pd.DataFrame, X_test: pd.DataFrame) -> np.ndarray:\n",
    "    preprocessor = preprocessing_pipeline()\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "    return X_train_preprocessed, X_test_preprocessed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ddec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n",
      "✅ data cleaned\n"
     ]
    }
   ],
   "source": [
    "df = clean_data(df)\n",
    "# Extract X and y\n",
    "X = df.drop(columns=['fraudulent'])\n",
    "y = df['fraudulent']\n",
    "# Make train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# preprocess train and test data\n",
    "X_train_preprocessed, X_test_preprocessed = train_preprocessor(X_train, X_test)\n",
    "# X_test_preprocessed = test_preprocessor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b495533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9738533643916376)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = clean_data(df)\n",
    "# Extract X and y\n",
    "X = df.drop(columns=['fraudulent'])\n",
    "y = df['fraudulent']\n",
    "# Make train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# preprocess train and test data\n",
    "X_train_preprocessed, X_test_preprocessed = train_preprocessor(X_train, X_test)\n",
    "# X_test_preprocessed = test_preprocessor(X_test)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    # (\"clean_preproc\", clean_preproc),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "cross_val_score(pipe, X_train_preprocessed, y_train, cv=cv).mean()\n",
    "\n",
    "cross_model = cross_validate(pipe, X_train_preprocessed, y_train, cv=cv)\n",
    "cross_model\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "result = model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "result.score\n",
    "\n",
    "y_pred = result.predict(X_test_preprocessed)\n",
    "\n",
    "print(recall_score(y_test, y_pred), precision_score(y_test, y_pred), accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f82b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.61700773, 0.65386415, 0.56823897, 0.60018778, 0.63627195]),\n",
       " 'score_time': array([0.00219345, 0.00320959, 0.0033884 , 0.00193262, 0.00251102]),\n",
       " 'test_score': array([0.97413492, 0.97553303, 0.97203775, 0.9751835 , 0.97237762])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60bf0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ClassifierMixin.score of LogisticRegression(max_iter=1000)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "result = model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "result.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "665620d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5606936416184971 0.97 0.9779082774049217\n"
     ]
    }
   ],
   "source": [
    "y_pred = result.predict(X_test_preprocessed)\n",
    "\n",
    "print(recall_score(y_test, y_pred), precision_score(y_test, y_pred), accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d6a59c",
   "metadata": {},
   "source": [
    "# Thoughts\n",
    "\n",
    "- For the basic ML Pipeline, we vectorize each of the descriptions seperately, add these vectors along with metainformation to train the model.\n",
    "- For deep learning, we will create different paths\n",
    "    1. Creating one document per entry by merging all information in a systematic way.\n",
    "    2. Creating a meta-data sentence, and leave all description parts seperated. We will then train one moddel per part. At theend we will train a model taking the probabilities for each part to get to a desion.\n",
    "    3. Same as 2. just that we will train one model taking all parts as inputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scam_job_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
